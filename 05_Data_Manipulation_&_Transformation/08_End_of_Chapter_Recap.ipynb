{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6442e2b7-f6cf-4782-bf0c-7a949286d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32793e3d-3408-4786-ac26-789dbf34c9d2",
   "metadata": {},
   "source": [
    "# End of Chapter Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bbc62-b882-41ae-ad3b-3749aaff46ae",
   "metadata": {},
   "source": [
    "## 1. Adding & Removing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc381f9a-6902-40cf-ac5b-5b9ed07b1c16",
   "metadata": {},
   "source": [
    "1. Create new columns:\n",
    "    1. Direct assignment - `df[\"col_name\"] = [....]` or `df[\"Bonus\"] = df[\"Salary\"] * 0.10`\n",
    "    2. Using `assign()` - `df.assign(Col_Name=[...])`\n",
    "    3. Using np.where - `df[\"Level\"] = np.where(df[\"Experience\"]>=3, \"Senior\", \"Junior\")`\n",
    "    4. Using `apply()` - `df[\"Tax\"] = df[\"Existing_col (Salary)\"].apply(lambda x: x * 0.05)`\n",
    "2. Deleting columna:\n",
    "    1. Single column - `df.drop(\"Col_name\", axis=1)`\n",
    "    2. Multiple column - `df.drop([\"col1\", \"col2\",...], axis=1)`\n",
    "    3. For inplace - Pass extra parameter `inplace=True`\n",
    "    4. Using `del` - `del df[\"col\"]` (it deletes inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48089c57-16ca-4824-84ac-5336b95e4a18",
   "metadata": {},
   "source": [
    "## 2. Sorting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36959332-44a0-4a89-b0c1-1b4b1922c5d9",
   "metadata": {},
   "source": [
    "1. `.sort_values()`:\n",
    "    1. `df.sort_values(\"Col_Name\")` - Ascending order\n",
    "    2. `df.sort_values(\"Col_Name\", ascending=False)` - Descending order\n",
    "    3. `df.sort_values([\"Col1\", \"col2\",...])` - Multiple columns -- First col1 them col2 ...\n",
    "    3. `df.sort_values([\"Col1\", \"col2\",...], ascending=[True, False])` - Multiple columns with different orders -- First col1 them col2 ...\n",
    "    4. `.sort_values(\"col\", na_position=\"first\"/\"last\")` - Na values either at first or at last\n",
    "2. `.sort_index()` - Ascending and `.sort_index(ascending=False)` - Descending\n",
    "\n",
    "For changes in original df use `inplace=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7a206-8dab-477d-ba7c-0d374041a8a5",
   "metadata": {},
   "source": [
    "## 3. Reindexing & Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161f8ff-c521-4cf8-9e19-64e84eba739b",
   "metadata": {},
   "source": [
    "Reindexing: changing order of rows & colns, adding new inded labels & removing existing ones.  \n",
    "1. `.reindex()` Rows:  \n",
    "Parameters:\n",
    "    1. List/Series - `[0, 1, 2, 3,...]` or `[\"a\", \"b\", \"c\",....]`\n",
    "    2. fill_value - `fill_value=0` or `fill_value=\"!\"` -- It fill missing index values to 0 / \"!\".\n",
    "\n",
    "2. `.reindex()` Columns:\n",
    "Parameters:\n",
    "    1. columns - `columns=[\"Name\", \"Marks\"]`\n",
    "    2. fill_value - `fill_value=0` or `fill_value=\"!\"` -- It fill missing column values to 0 / \"!\".\n",
    "3. Aligning Data:  \n",
    "If we have two series s1 & s2. And if we do s1 + s2. The operation is done only on matching index rows.  \n",
    "All other rows became NaN.  \n",
    "This mean the data between two series are aligned by index. More precise on common index.\n",
    "5. Align DataFrame:  \n",
    "DataFrame alignment are done by `.align()` method.  \n",
    "`df1.align(df2)` -> This returns two df's. Take it as left and right. left-df1 & right-df2.  \n",
    "So `left, right = df1.align(df2)`  \n",
    "Here also the alignment done on the basis of index.  \n",
    "All the data from df2 is NaN in df1 and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2bca9-530e-4c9a-8db3-a3f6229a84f7",
   "metadata": {},
   "source": [
    "## 4. Merging, Joining & Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd22db-1565-4bd1-ad63-ee2dc22fdd1d",
   "metadata": {},
   "source": [
    "**Part 1: `pd.merge()` (SQL style joins)**\n",
    "1. Inner join (default): `pd.merge(df1, df2, on=\"common_col\")`\n",
    "2. Left join : `pd.merge(df1, df2, on=\"common_col\", how=\"left\")`\n",
    "3. Right join : `pd.merge(df1, df2, on=\"common_col\", how=\"right\")`\n",
    "4. Right join : `pd.merge(df1, df2, on=\"common_col\", how=\"outer\")`\n",
    "5. Right join : `pd.merge(df1, df2, left_on=\"common_col\", right_on=\"common_col\")`\n",
    "\n",
    "**Part 2: `.join()` (Index-based joining)**  \n",
    "The df1 has to be indexed to common column.  \n",
    "`.join()` is used when we want to join df on a index.  \n",
    "So before that we have to do `df1.set_index(\"common_col_df1\", inplace=True)`  \n",
    "1. `df1.join(df2, on=\"common_col_df2\")`\n",
    "\n",
    "**Part 3: `pd.concat()` (Stacking DataFrames)**  \n",
    "1. Row-wise concatenation (default) -  \n",
    "Column concat with common column and if no common then values are filled NaN.  \n",
    "`pd.concat([df1, df2])` - Get index as existed earlier in df1/df2  \n",
    "`pd.concat([df1, df2], ignore_index=True)` - This sets fresh indexs\n",
    "So the df's are concatenated row-wise with common columns and other columns are filled with NaN\n",
    "\n",
    "2. Colimn-wise concatenation -     \n",
    "`pd.concar([df1,df2], axis=1)`  \n",
    "Concatenated by common index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411c6f8-c311-4925-b42e-d13a6f024b44",
   "metadata": {},
   "source": [
    "## 5. GroupBy & Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be4e0b-6264-40f3-9aaf-d9b396eed52b",
   "metadata": {},
   "source": [
    "split->apply->combine  \n",
    "1. Basic `groupby()` - One column\n",
    "    1. `df.groupby(\"col1\")[\"col2\"].mean()`  \n",
    "        Group by \"col1\", aggregate function on \"col2\" and mean() agg fun is applied.\n",
    "    2. Agg functions - mean, avg, count, min, max, sum,...\n",
    "2. Multiple agg function\n",
    "   1. `df.groupby(\"col1\")[\"col2\"].agg[\"mean\", \"sum\", \"max\",...]`  \n",
    "      Multiple agg functions are applied in a single statement.\n",
    "   2. ```\n",
    "        df.groupby(\"col1\").agg(\n",
    "            col_name=(\"col2\", \"agg_fun\"),\n",
    "            ...\n",
    "        )\n",
    "      ```\n",
    "3. `groupby()` multiple columns\n",
    "    1. `df.groupby([\"col1\", ....])[\"col\"].mean()`\n",
    "    2. ```\n",
    "        df.groupby([\"col1\", \"col2\"]).agg(\n",
    "            col_name=(\"col3\", \"agg_fun\"),\n",
    "            ...\n",
    "        )\n",
    "        ```\n",
    "4. `transform()`  \n",
    "    `df[\"col_name\"] = df.groupby()[].transform(\"mean\")` - It gives same amount of rows as original.  \n",
    "   Shape stays as the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebd0b7-967f-43e1-8c0f-cc2938c9cccc",
   "metadata": {},
   "source": [
    "## 6. Pivot tables & Crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cacd16-e567-4d0f-aeca-675cb2560539",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "```\n",
    "pd.pivot_table(\n",
    "    df,\n",
    "    values = \"..\" / [List],\n",
    "    index = \"..\", [List]\n",
    "    columns = \"..\" / [List],\n",
    "    aggfunc = \"..\" / [List]\n",
    ")\n",
    "```  \n",
    "Groups the data and aggregate the values in a table  \n",
    "1. **Parameters:**:\n",
    "    1. df -> DataFrame in which the all data available\n",
    "    2. values -> Values on which the aggregate function is applied\n",
    "    3. index -> row or list of rows w wanted in result table\n",
    "    4. columns -> column or list of column wanted in result\n",
    "    5. aggfunc -> single function or list of functions to be apply on values\n",
    "\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "```\n",
    "pd.crosstab(\n",
    "    df[\"..\"],\n",
    "    df[\"..\"],\n",
    "    normalize = True / False / \"index\"\n",
    ")\n",
    "```\n",
    "It makes the frequencies count of every pair occured.  \n",
    "In two columns it gives **\"How many times does each pair occured\"**\n",
    "1. **Parameters:**\n",
    "    1. First column\n",
    "    2. Second columns\n",
    "    3. Normalize ->\n",
    "        1. True -> Overall percentage\n",
    "        2. \"index\" -> index/row wise percentage\n",
    "        3. \"columns\" -> column wise percentage\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0467e-f6de-4c49-a2ed-5a2788ce5ab0",
   "metadata": {},
   "source": [
    "## 7. Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01675c9d-bdd3-423e-917b-4d0a45ad836c",
   "metadata": {},
   "source": [
    "1. `melt()` -> It converts columns into rows.  \n",
    "   - id_vars - Columns want to keep as same.  \n",
    "   - value_vars - Columns want to unpivot, If not specified then all columns which are not specified in id_vars.  \n",
    "   - var_name - Name of the column for column names.  \n",
    "   - value_name - Name of column for column values.  \n",
    "   - ignore_index - True: fresh index (default), False: Original index  \n",
    "2. `stack()` -> It keeps index column as it is (not at all) and all other columns are converted into rows.  \n",
    "   It returns series. We have to convert it to df.  \n",
    "4. `unstack()` -> Converts the stacked series into original df.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
